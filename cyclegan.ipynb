{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-25T10:50:42.394426Z","iopub.execute_input":"2021-11-25T10:50:42.394725Z","iopub.status.idle":"2021-11-25T10:50:50.023904Z","shell.execute_reply.started":"2021-11-25T10:50:42.394697Z","shell.execute_reply":"2021-11-25T10:50:50.022942Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\npath = '/kaggle/input/clean-dirty-containers-in-montevideo/clean-dirty-garbage-containers-V4/clean-dirty-garbage-containers'\n#eval_path = '/kaggle/input/clean-dirty-containers-in-montevideo/clean-dirty-garbage-containers-V4/clean-dirty-garbage-containers/test'\n#weigths='/kaggle/input/keras-pretrain-model-weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-11-25T10:50:50.025747Z","iopub.execute_input":"2021-11-25T10:50:50.026096Z","iopub.status.idle":"2021-11-25T10:50:54.453791Z","shell.execute_reply.started":"2021-11-25T10:50:50.026058Z","shell.execute_reply":"2021-11-25T10:50:54.453060Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nimport os\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport PIL.Image as Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:50:54.455035Z","iopub.execute_input":"2021-11-25T10:50:54.455406Z","iopub.status.idle":"2021-11-25T10:50:56.291919Z","shell.execute_reply.started":"2021-11-25T10:50:54.455371Z","shell.execute_reply":"2021-11-25T10:50:56.291001Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    \n    def __init__(self,path):\n        super().__init__()\n        self.path = path\n        self.train = os.listdir(path)\n        self.image_transforms = transforms.Compose([transforms.Resize((224, 224)),\n                                                    transforms.ToTensor(),\n                                                    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)),])\n   \n    def __len__(self):\n        return len(self.train)\n    \n    def __getitem__(self,idx):\n        image = f'{self.path}/{self.train[idx]}'\n        image = Image.open(image)\n        if self.image_transforms:\n            return self.image_transforms(image)\n        return image\n    \n\ndef make_dataloader(batch_size=8, **kwargs):\n    dataset = ImageDataset(**kwargs)\n    dataloader = DataLoader(dataset,batch_size=batch_size,num_workers=1,pin_memory=True,shuffle=True)\n    return dataloader  ","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:50:56.293499Z","iopub.execute_input":"2021-11-25T10:50:56.293818Z","iopub.status.idle":"2021-11-25T10:50:56.305087Z","shell.execute_reply.started":"2021-11-25T10:50:56.293780Z","shell.execute_reply":"2021-11-25T10:50:56.304283Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataloader_traindirty = make_dataloader(path=f'{path}/train/dirty')\ndataloader_trainclean = make_dataloader(path=f'{path}/train/clean')\ndataloader_testclean = make_dataloader(path=f'{path}/test/clean')\ndataloader_testdirty = make_dataloader(path=f'{path}/test/dirty')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:50:56.309169Z","iopub.execute_input":"2021-11-25T10:50:56.309486Z","iopub.status.idle":"2021-11-25T10:50:56.327055Z","shell.execute_reply.started":"2021-11-25T10:50:56.309459Z","shell.execute_reply":"2021-11-25T10:50:56.326186Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def imshow(grid,title):\n    nparray = grid.cpu().numpy()\n    nparray = (nparray*0.5)+0.5\n    plt.figure(figsize=(12,12))\n    plt.title(title)\n    plt.axis('off')\n    plt.imshow(np.transpose(nparray,(1,2,0)))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:50:56.328654Z","iopub.execute_input":"2021-11-25T10:50:56.329148Z","iopub.status.idle":"2021-11-25T10:50:56.335777Z","shell.execute_reply.started":"2021-11-25T10:50:56.329109Z","shell.execute_reply":"2021-11-25T10:50:56.334980Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"imgsA = next(iter(dataloader_trainclean))\ngridA = torchvision.utils.make_grid(imgsA,nrow=4)\nimshow(gridA,'Clean Image')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:50:56.337138Z","iopub.execute_input":"2021-11-25T10:50:56.337523Z","iopub.status.idle":"2021-11-25T10:51:02.399611Z","shell.execute_reply.started":"2021-11-25T10:50:56.337465Z","shell.execute_reply":"2021-11-25T10:51:02.397167Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"imgsA = next(iter(dataloader_traindirty))\ngridA = torchvision.utils.make_grid(imgsA,nrow=4)\nimshow(gridA,'Dirty Image')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:02.400928Z","iopub.execute_input":"2021-11-25T10:51:02.401270Z","iopub.status.idle":"2021-11-25T10:51:03.229221Z","shell.execute_reply.started":"2021-11-25T10:51:02.401233Z","shell.execute_reply":"2021-11-25T10:51:03.228164Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n  \n    def __init__(self,in_channels,out_channels,down=True,act=True,**kwargs):\n        super().__init__()\n        self.conv = nn.Sequential(\n                             nn.Conv2d(in_channels=in_channels,out_channels=out_channels,padding_mode='reflect',**kwargs)\n                             if down else nn.ConvTranspose2d(in_channels=in_channels,out_channels=out_channels,**kwargs),\n                             nn.InstanceNorm2d(out_channels),\n                             nn.ReLU(inplace=True) if act else nn.Identity(),\n                                )\n\n    def forward(self,x):\n        return self.conv(x)\n    \n\nclass ResidualBlock(nn.Module):\n  \n    def __init__(self,channels,**kwargs):\n        super().__init__()\n        self.main = nn.Sequential(\n                             ConvBlock(channels,channels,kernel_size=3, padding=1),\n                             ConvBlock(channels,channels,act=False,kernel_size=3, padding=1)\n                                 )\n\n    def forward(self,x):\n        return x + self.main(x)\n    \n\nclass Generator(nn.Module):\n\n    def __init__(self,nb_features,nb_residualblock=9):\n        super().__init__()\n        self.first = ConvBlock(3,nb_features,kernel_size=7,stride=1,padding=3)\n        self.down = nn.ModuleList(\n                        [ConvBlock(nb_features,nb_features*2,kernel_size=3, stride=2, padding=1),\n                        ConvBlock(nb_features*2,nb_features*4,kernel_size=3, stride=2, padding=1)]\n                                 )\n        self.residual_block = nn.Sequential(*[ResidualBlock(nb_features*4) for _ in range(nb_residualblock)])\n        self.up = nn.ModuleList(\n                        [ConvBlock(nb_features*4,nb_features*2,down=False,kernel_size=3, stride=2, padding=1, output_padding=1),\n                        ConvBlock(nb_features*2,nb_features,down=False,kernel_size=3, stride=2, padding=1, output_padding=1)]\n                               )\n        self.last = nn.Conv2d(nb_features,3,kernel_size=7,stride=1,padding=3, padding_mode=\"reflect\")\n\n    def forward(self,x):\n        x = self.first(x)\n        for layer in self.down:\n            x = layer(x)\n        x = self.residual_block(x)\n        for layer in self.up:\n            x = layer(x)\n        x = nn.Tanh()(self.last(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:53:46.934096Z","iopub.execute_input":"2021-11-25T10:53:46.935268Z","iopub.status.idle":"2021-11-25T10:53:46.970213Z","shell.execute_reply.started":"2021-11-25T10:53:46.935211Z","shell.execute_reply":"2021-11-25T10:53:46.969020Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n\n    def __init__(self,in_channels,out_channels,stride):\n        super().__init__()\n        self.conv = nn.Sequential(\n                          nn.Conv2d(in_channels,out_channels,kernel_size=4,stride=stride,padding=1,padding_mode='reflect'),\n                          nn.InstanceNorm2d(out_channels),\n                          nn.LeakyReLU(0.2,inplace=True),\n                             )\n    def forward(self,x):\n        return self.conv(x)\n    \n\nclass Discriminator(nn.Module):\n\n    def __init__(self,nb_features):\n        super().__init__()\n        self.first = nn.Sequential(\n                         nn.Conv2d(3,nb_features,kernel_size=4,stride=2,padding=1,padding_mode='reflect'),\n                         nn.LeakyReLU(0.2,inplace=True),\n                              )\n        self.convblock = nn.ModuleList(\n                              [Block(nb_features,nb_features*2,2),\n                               Block(nb_features*2,nb_features*4,2),\n                               Block(nb_features*4,nb_features*6,1)]\n                                  )\n        self.last = nn.Sequential(\n                        nn.Conv2d(nb_features*6,1,kernel_size=4,padding=1,padding_mode='reflect'),\n                        nn.Sigmoid()\n                             )\n    def forward(self,x):\n        x = self.first(x)\n        for layer in self.convblock:    \n            x = layer(x)\n        x = self.last(x)\n        return x    ","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:54:02.295522Z","iopub.execute_input":"2021-11-25T10:54:02.295851Z","iopub.status.idle":"2021-11-25T10:54:02.308333Z","shell.execute_reply.started":"2021-11-25T10:54:02.295820Z","shell.execute_reply":"2021-11-25T10:54:02.307471Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class CycleGAN(nn.Module):\n    \n    def __init__(self,nb_features=64,lr=0.002,beta1=0.5,beta2=0.999,lambda_cycle=10.0):\n        super().__init__()\n        self.G_AB = torch.load(\"../input/cyclegan/CycleGAN/Generator_AtoB.pth\").to(device)\n        self.G_BA = torch.load(\"../input/cyclegan/CycleGAN/Generator_BtoA.pth\").to(device)\n        self.D_A  = torch.load(\"../input/cyclegan/CycleGAN/Discriminator_A.pth\").to(device)\n        self.D_B  = torch.load(\"../input/cyclegan/CycleGAN/Discriminator_B.pth\").to(device)\n        self.adversarial_loss = nn.MSELoss()\n        self.cycle_loss = nn.L1Loss()\n        self.opt_G = torch.optim.Adam(itertools.chain(self.G_AB.parameters(),self.G_BA.parameters()),lr=lr,betas=(beta1,beta2))\n        self.opt_D_A  = torch.optim.Adam(self.D_A.parameters(),lr=lr,betas=(beta1,beta2))\n        self.opt_D_B  = torch.optim.Adam(self.D_B.parameters(),lr=lr,betas=(beta1,beta2))\n        self.lambda_cycle = lambda_cycle\n    \n    def setup_input(self,real_A,real_B):\n        self.real_A = real_A.to(device)\n        self.real_B = real_B.to(device)\n        self.fake_A = self.G_BA(self.real_B)\n        self.fake_B = self.G_AB(self.real_A)\n    \n    def optimize_D(self):\n        self.D_A.train()\n        self.D_B.train()\n        real_preds = self.D_A(self.real_A)\n        fake_preds = self.D_A(self.fake_A.detach())\n        real_loss = self.adversarial_loss(real_preds,torch.ones_like(real_preds,device=device))\n        fake_loss = self.adversarial_loss(fake_preds,torch.zeros_like(fake_preds,device=device))\n        loss_D_A = (real_loss+fake_loss)/2\n        \n        real_preds = self.D_B(self.real_B)\n        fake_preds = self.D_B(self.fake_B.detach())\n        real_loss = self.adversarial_loss(real_preds,torch.ones_like(real_preds,device=device))\n        fake_loss = self.adversarial_loss(fake_preds,torch.zeros_like(fake_preds,device=device))\n        loss_D_B = (real_loss+fake_loss)/2\n    \n        self.opt_D_A.zero_grad()\n        loss_D_A.backward()\n        self.opt_D_A.step()\n        \n        self.opt_D_B.zero_grad()\n        loss_D_B.backward()\n        self.opt_D_B.step()\n        \n        return loss_D_A,loss_D_B\n        \n    def optimize_G(self):\n        self.G_AB.train()\n        self.G_BA.train()\n        fake_preds_A = self.D_A(self.fake_A)\n        fake_preds_B = self.D_B(self.fake_B)\n        adversarial_loss_G_AB = self.adversarial_loss(fake_preds_B,torch.ones_like(fake_preds_B,device=device))\n        adversarial_loss_G_BA = self.adversarial_loss(fake_preds_A,torch.ones_like(fake_preds_A,device=device))\n        adversarial_loss_G = (adversarial_loss_G_AB + adversarial_loss_G_BA)/2\n        \n        cycle_loss_G_AB = self.cycle_loss(self.real_A,self.G_BA(self.fake_B))\n        cycle_loss_G_BA = self.cycle_loss(self.real_B,self.G_AB(self.fake_A))\n        cycle_loss_G = (cycle_loss_G_AB + cycle_loss_G_BA)/2\n        loss_G = adversarial_loss_G + (self.lambda_cycle*cycle_loss_G)\n    \n        self.opt_G.zero_grad()\n        loss_G.backward()\n        self.opt_G.step()\n        \n        return loss_G","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:54:13.279790Z","iopub.execute_input":"2021-11-25T10:54:13.280147Z","iopub.status.idle":"2021-11-25T10:54:13.304965Z","shell.execute_reply.started":"2021-11-25T10:54:13.280113Z","shell.execute_reply":"2021-11-25T10:54:13.304054Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"gan = CycleGAN()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:54:19.661741Z","iopub.execute_input":"2021-11-25T10:54:19.662099Z","iopub.status.idle":"2021-11-25T10:54:19.744623Z","shell.execute_reply.started":"2021-11-25T10:54:19.662058Z","shell.execute_reply":"2021-11-25T10:54:19.739930Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# helper conv function\ndef conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n    \"\"\"Creates a convolutional layer, with optional batch normalization.\n    \"\"\"\n    layers = []\n    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n    \n    layers.append(conv_layer)\n\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n    return nn.Sequential(*layers)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.230788Z","iopub.execute_input":"2021-11-25T10:51:03.237347Z","iopub.status.idle":"2021-11-25T10:51:03.252553Z","shell.execute_reply.started":"2021-11-25T10:51:03.237304Z","shell.execute_reply":"2021-11-25T10:51:03.251826Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \n    def __init__(self, conv_dim=64):\n        super(Discriminator, self).__init__()\n\n        # Define all convolutional layers\n        # Should accept an RGB image as input and output a single value\n        \n        # 1st Conv layer has no batch_norm. dim = [64, 64, 64]\n        self.conv1 = conv(3, conv_dim, 4, batch_norm=False)\n        # 2nd Conv layer. dim = [32, 32, 128]\n        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n        # 3rd Conv layer. dim = [16, 16, 256]\n        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n        # 4th Conv layer. dim = [8, 8, 512]\n        self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n        # 5th Last Conv layer, Classification Layer has no batch_norm and stride is 1. dim = [1, 1, 1]\n        self.conv5 = conv(conv_dim*8, 1, 4, stride=1, batch_norm=False)\n\n    def forward(self, x):\n        # feedforward behavior - relu to all but last layer\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = self.conv5(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.253887Z","iopub.execute_input":"2021-11-25T10:51:03.254378Z","iopub.status.idle":"2021-11-25T10:51:03.279085Z","shell.execute_reply.started":"2021-11-25T10:51:03.254340Z","shell.execute_reply":"2021-11-25T10:51:03.278222Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# residual block class\nclass ResidualBlock(nn.Module):\n    \"\"\"Defines a residual block.\n       This adds an input x to a convolutional layer (applied to x) with the same size input and output.\n       These blocks allow a model to learn an effective transformation from one domain to another.\n    \"\"\"\n    def __init__(self, conv_dim):\n        super(ResidualBlock, self).__init__()\n        # conv_dim = number of inputs  \n        \n        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\n        # layers should have the same shape input as output; I suggest a kernel_size of 3\n        self.conv1 = conv(conv_dim, conv_dim, 3, 1, batch_norm=True)\n        self.conv2 = conv(conv_dim, conv_dim, 3, 1, batch_norm=True)\n        \n    def forward(self, x):\n        # apply a ReLu activation the outputs of the first layer\n        # return a summed output, x + resnet_block(x)\n        output = F.relu(self.conv1(x))\n        output = x + self.conv2(output)\n        \n        return output\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.284051Z","iopub.execute_input":"2021-11-25T10:51:03.286368Z","iopub.status.idle":"2021-11-25T10:51:03.296641Z","shell.execute_reply.started":"2021-11-25T10:51:03.286331Z","shell.execute_reply":"2021-11-25T10:51:03.295802Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CycleGenerator(nn.Module):\n    \n    def __init__(self, conv_dim=64, n_res_blocks=6):\n        super(CycleGenerator, self).__init__()\n\n        # 1. Define the encoder part of the generator\n        self.conv1 = conv(3, conv_dim, 4)\n        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n\n        # 2. Define the resnet part of the generator\n        res_layers = []\n        for layer in range(n_res_blocks):\n          res_layers.append(ResidualBlock(conv_dim*4))\n\n        self.resblocks = nn.Sequential(*res_layers)\n\n        # 3. Define the decoder part of the generator\n        self.deconv1 = deconv(conv_dim*4, conv_dim*2, 4)\n        self.deconv2 = deconv(conv_dim*2, conv_dim, 4)\n        self.deconv3 = deconv(conv_dim, 3, 4, batch_norm=False)\n\n    def forward(self, x):\n        \"\"\"Given an image x, returns a transformed image.\"\"\"\n        # define feedforward behavior, applying activations as necessary\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n\n        x = self.resblocks.forward(x)\n\n        x = F.relu(self.deconv1(x))\n        x = F.relu(self.deconv2(x))\n        x = F.tanh(self.deconv3(x))\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.297925Z","iopub.execute_input":"2021-11-25T10:51:03.298286Z","iopub.status.idle":"2021-11-25T10:51:03.322927Z","shell.execute_reply.started":"2021-11-25T10:51:03.298250Z","shell.execute_reply":"2021-11-25T10:51:03.322026Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\n    \"\"\"Builds the generators and discriminators.\"\"\"\n    \n    # Instantiate generators\n    G_XtoY = CycleGenerator(conv_dim=g_conv_dim, n_res_blocks=n_res_blocks)\n    G_YtoX = CycleGenerator(conv_dim=g_conv_dim, n_res_blocks=n_res_blocks)\n    # Instantiate discriminators\n    D_X = Discriminator(conv_dim=d_conv_dim)\n    D_Y = Discriminator(conv_dim=d_conv_dim)\n\n    # move models to GPU, if available\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        G_XtoY.to(device)\n        G_YtoX.to(device)\n        D_X.to(device)\n        D_Y.to(device)\n        print('Models moved to GPU.')\n    else:\n        print('Only CPU available.')\n\n    return G_XtoY, G_YtoX, D_X, D_Y","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.327010Z","iopub.execute_input":"2021-11-25T10:51:03.328105Z","iopub.status.idle":"2021-11-25T10:51:03.338444Z","shell.execute_reply.started":"2021-11-25T10:51:03.328066Z","shell.execute_reply":"2021-11-25T10:51:03.337499Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# helper deconv function\ndef deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n    \"\"\"Creates a transpose convolutional layer, with optional batch normalization.\n    \"\"\"\n    layers = []\n    # append transpose conv layer\n    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n    # optional batch norm layer\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n    return nn.Sequential(*layers)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.340068Z","iopub.execute_input":"2021-11-25T10:51:03.340752Z","iopub.status.idle":"2021-11-25T10:51:03.349055Z","shell.execute_reply.started":"2021-11-25T10:51:03.340711Z","shell.execute_reply":"2021-11-25T10:51:03.348193Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# call the function to get models\nG_XtoY, G_YtoX, D_X, D_Y = create_model()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.350520Z","iopub.execute_input":"2021-11-25T10:51:03.351330Z","iopub.status.idle":"2021-11-25T10:51:03.735591Z","shell.execute_reply.started":"2021-11-25T10:51:03.351281Z","shell.execute_reply":"2021-11-25T10:51:03.734714Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# helper function for printing the model architecture\ndef print_models(G_XtoY, G_YtoX, D_X, D_Y):\n    \"\"\"Prints model information for the generators and discriminators.\n    \"\"\"\n    print(\"                     G_XtoY                    \")\n    print(\"-----------------------------------------------\")\n    print(G_XtoY)\n    print()\n\n    print(\"                     G_YtoX                    \")\n    print(\"-----------------------------------------------\")\n    print(G_YtoX)\n    print()\n\n    print(\"                      D_X                      \")\n    print(\"-----------------------------------------------\")\n    print(D_X)\n    print()\n\n    print(\"                      D_Y                      \")\n    print(\"-----------------------------------------------\")\n    print(D_Y)\n    print()\n    \n\n# print all of the models\nprint_models(G_XtoY, G_YtoX, D_X, D_Y)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.739763Z","iopub.execute_input":"2021-11-25T10:51:03.741869Z","iopub.status.idle":"2021-11-25T10:51:03.761154Z","shell.execute_reply.started":"2021-11-25T10:51:03.741827Z","shell.execute_reply":"2021-11-25T10:51:03.760372Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def real_mse_loss(D_out):\n    # how close is the produced output from being \"real\"?\n    return torch.mean((D_out-1)**2)\n\ndef fake_mse_loss(D_out):\n    # how close is the produced output from being \"fake\"?\n    return torch.mean((D_out-0)**2)\n\ndef cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\n    # calculate reconstruction loss \n    reconstr_loss = torch.mean(torch.abs(real_im - reconstructed_im))\n    # return weighted loss\n    return lambda_weight*reconstr_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.764961Z","iopub.execute_input":"2021-11-25T10:51:03.767054Z","iopub.status.idle":"2021-11-25T10:51:03.775826Z","shell.execute_reply.started":"2021-11-25T10:51:03.767001Z","shell.execute_reply":"2021-11-25T10:51:03.774970Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# hyperparams for Adam optimizers\nlr=0.0002\nbeta1=0.5\nbeta2=0.999\n\ng_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n\n# Create optimizers for the generators and discriminators\ng_optimizer = optim.Adam(g_params, lr, [beta1, beta2])\nd_x_optimizer = optim.Adam(D_X.parameters(), lr, [beta1, beta2])\nd_y_optimizer = optim.Adam(D_Y.parameters(), lr, [beta1, beta2])","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:03.780863Z","iopub.execute_input":"2021-11-25T10:51:03.783527Z","iopub.status.idle":"2021-11-25T10:51:03.797133Z","shell.execute_reply.started":"2021-11-25T10:51:03.783473Z","shell.execute_reply":"2021-11-25T10:51:03.796219Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# train the network\ndef training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \n                  n_epochs=1000):\n    \n    print_every=10\n    \n    # keep track of losses over time\n    losses = []\n\n    test_iter_X = iter(test_dataloader_X)\n    test_iter_Y = iter(test_dataloader_Y)\n\n    # Get some fixed data from domains X and Y for sampling. These are images that are held\n    # constant throughout training, that allow us to inspect the model's performance.\n    #fixed_X = test_iter_X.next()[0]\n    #fixed_Y = test_iter_Y.next()[0]\n    #fixed_X = scale(fixed_X) # make sure to scale to a range -1 to 1\n    #fixed_Y = scale(fixed_Y)\n\n    # batches per epoch\n    iter_X = iter(dataloader_X)\n    iter_Y = iter(dataloader_Y)\n    batches_per_epoch = min(len(iter_X), len(iter_Y))\n\n    for epoch in range(1, n_epochs+1):\n\n        # Reset iterators for each epoch\n        if epoch % batches_per_epoch == 0:\n            iter_X = iter(dataloader_X)\n            iter_Y = iter(dataloader_Y)\n\n        images_X, _ = iter_X.next()\n        #images_X = scale(images_X) # make sure to scale to a range -1 to 1\n\n        images_Y, _ = iter_Y.next()\n        #images_Y = scale(images_Y)\n        \n        # move images to GPU if available (otherwise stay on CPU)\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        images_X = images_X.to(device)\n        images_Y = images_Y.to(device)\n\n\n        # ============================================\n        #            TRAIN THE DISCRIMINATORS\n        # ============================================\n\n        ##   First: D_X, real and fake loss components   ##\n        d_x_optimizer.zero_grad()\n        # 1. Compute the discriminator losses on real images\n        out_x = D_X(images_X)\n        D_X_real_loss = real_mse_loss(out_x)\n\n        # 2. Generate fake images that look like domain X based on real images in domain Y\n        fake_x = G_YtoX(images_Y)\n\n        # 3. Compute the fake loss for D_X\n        out_x = D_X(fake_x)\n        D_X_fake_loss = fake_mse_loss(out_x)\n\n        # 4. Compute the total loss and perform backprop\n        d_x_loss = D_X_real_loss + D_X_fake_loss\n        d_x_loss.backward(retain_graph=True)\n        d_x_optimizer.step()\n        \n        ##   Second: D_Y, real and fake loss components   ##\n        d_y_optimizer.zero_grad()\n\n        # 1. Compute the discriminator losses on real images\n        out_y = D_Y(images_Y)\n        D_Y_real_loss = real_mse_loss(out_y)\n\n        # 2. Generate fake images that look like domain Y based on real images in domain X\n        fake_y = G_XtoY(images_X)\n\n        # 3. Compute the fake loss for D_Y\n        out_y = D_Y(fake_y)\n        D_Y_fake_loss = fake_mse_loss(out_y)\n\n        # 4. Compute the total loss and perform backprop\n        d_y_loss = D_Y_real_loss + D_Y_fake_loss\n        d_y_loss.backward(retain_graph=True)\n        d_y_optimizer.step()\n        \n\n        # =========================================\n        #            TRAIN THE GENERATORS\n        # =========================================\n\n        ##    First: generate fake X images and reconstructed Y images    ##\n        g_optimizer.zero_grad()\n        \n        # 1. Generate fake images that look like domain X based on real images in domain Y\n        fake_X = G_YtoX(images_Y)\n\n        # 2. Compute the generator loss based on domain X\n        out_x = D_X(fake_X)\n        g_YtoX_loss = real_mse_loss(out_x)\n\n        # 3. Create a reconstructed y\n        reconstructed_Y = G_XtoY(fake_X)\n\n        # 4. Compute the cycle consistency loss (the reconstruction loss)\n        reconstructed_y_loss = cycle_consistency_loss(images_Y, reconstructed_Y, lambda_weight=10)\n\n        ##    Second: generate fake Y images and reconstructed X images    ##\n         \n        # 1. Generate fake images that look like domain X based on real images in domain Y\n        fake_Y = G_XtoY(images_X)\n\n        # 2. Compute the generator loss based on domain X\n        out_x = D_Y(fake_Y)\n        g_XtoY_loss = real_mse_loss(out_y)\n\n        # 3. Create a reconstructed y\n        reconstructed_X = G_YtoX(fake_Y)\n\n        # 4. Compute the cycle consistency loss (the reconstruction loss)\n        reconstructed_x_loss = cycle_consistency_loss(images_X, reconstructed_X, lambda_weight=10)\n\n\n        # 5. Add up all generator and reconstructed losses and perform backprop\n        g_total_loss = g_YtoX_loss + g_XtoY_loss + reconstructed_y_loss + reconstructed_x_loss\n      \n        g_total_loss.backward(retain_graph=True)\n        g_optimizer.step()\n        \n        # Print the log info\n        if epoch % print_every == 0:\n            # append real and fake discriminator losses and the generator loss\n            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(\n                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n\n            \n        sample_every=100\n        # Save the generated samples\n        if epoch % sample_every == 0:\n            G_YtoX.eval() # set generators to eval mode for sample generation\n            G_XtoY.eval()\n            save_samples(epoch, G_YtoX, G_XtoY, batch_size=16)\n            G_YtoX.train()\n            G_XtoY.train()\n\n        # uncomment these lines, if you want to save your model\n#         checkpoint_every=1000\n#         # Save the model parameters\n#         if epoch % checkpoint_every == 0:\n#             checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n\n    return losses\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:52:26.977097Z","iopub.execute_input":"2021-11-25T10:52:26.977468Z","iopub.status.idle":"2021-11-25T10:52:26.999749Z","shell.execute_reply.started":"2021-11-25T10:52:26.977432Z","shell.execute_reply":"2021-11-25T10:52:26.998680Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"n_epochs = 10 # keep this small when testing if a model first works, then increase it to >=1000\nlosses = training_loop(dataloader_traindirty, dataloader_trainclean, dataloader_testdirty, dataloader_testclean, n_epochs=n_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:52:27.631366Z","iopub.execute_input":"2021-11-25T10:52:27.631675Z","iopub.status.idle":"2021-11-25T10:52:28.519079Z","shell.execute_reply.started":"2021-11-25T10:52:27.631646Z","shell.execute_reply":"2021-11-25T10:52:28.517193Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}